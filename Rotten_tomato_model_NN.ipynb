{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gIsjVHV18zm-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, PowerTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tqdm.auto import tqdm\n",
        "import re\n",
        "from datetime import datetime\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjuWkWy0eWOQ",
        "outputId": "825e6b02-cce4-4dc1-e1da-fcd032971a72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading dataset...\")\n",
        "dataset_path = \"/content/Rotten_Tomatoes_Movies dataset(1).csv\"\n",
        "df = pd.read_csv(dataset_path, header=0, index_col=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SggCUk5cwE72",
        "outputId": "f3e849da-6500-4d00-b366-8dfaaa7207e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-febd53810df4>:45: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  data[numerical_features] = data[numerical_features].fillna(0)\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/core/_methods.py:176: RuntimeWarning: overflow encountered in multiply\n",
            "  x = um.multiply(x, x, out=x)\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/core/_methods.py:187: RuntimeWarning: overflow encountered in reduce\n",
            "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "data = pd.read_csv(\"/content/Rotten_Tomatoes_Movies dataset(1).csv\")\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "data = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
        "\n",
        "# Bin audience ratings\n",
        "bins = np.linspace(0, 100, 21)\n",
        "labels = np.arange(20)\n",
        "data['audience_rating'] = pd.cut(data['audience_rating'], bins=bins, labels=labels, right=True).astype(float)\n",
        "data['audience_rating'] = data['audience_rating'].fillna(data['audience_rating'].median()).astype(int)\n",
        "\n",
        "# Encode categorical data\n",
        "rating_map = {'G': 1, 'PG': 2, 'PG-13': 3, 'R': 4, 'NC-17': 5, 'PG-13)': 3, 'R)': 4, 'NC17': 5, 'NR': 0}\n",
        "status_map = {'Rotten': 0, 'Fresh': 1, 'Certified Fresh': 2}\n",
        "data['rating'] = data['rating'].map(rating_map)\n",
        "data['tomatometer_status'] = data['tomatometer_status'].map(status_map)\n",
        "\n",
        "# Process date features\n",
        "data['in_theaters_date'] = pd.to_datetime(data['in_theaters_date'], format='%m/%d/%Y', errors='coerce')\n",
        "data['on_streaming_date'] = pd.to_datetime(data['on_streaming_date'], format='%m/%d/%Y', errors='coerce')\n",
        "data['days_in_theaters'] = (data['on_streaming_date'] - data['in_theaters_date']).dt.days\n",
        "data['release_year'] = data['in_theaters_date'].dt.year\n",
        "data['release_month'] = data['in_theaters_date'].dt.month\n",
        "data['release_quarter'] = data['in_theaters_date'].dt.quarter\n",
        "current_year = datetime.now().year\n",
        "data['movie_age'] = current_year - data['release_year']\n",
        "\n",
        "# Extract studio and genre features\n",
        "studio_size = data['studio_name'].value_counts()\n",
        "data['studio_size'] = data['studio_name'].map(studio_size)\n",
        "data['genre_count'] = data['genre'].fillna('').str.count(',') + 1\n",
        "\n",
        "data['cast_size'] = data['cast'].fillna('').str.count(',') + 1\n",
        "data['director_count'] = data['directors'].fillna('').str.count(',') + 1\n",
        "data['writer_count'] = data['writers'].fillna('').str.count(',') + 1\n",
        "\n",
        "# Drop unnecessary columns\n",
        "data = data.drop(['in_theaters_date', 'on_streaming_date'], axis=1)\n",
        "\n",
        "# Normalize numerical features\n",
        "numerical_features = ['runtime_in_minutes', 'tomatometer_rating', 'tomatometer_count', 'days_in_theaters', 'release_year', 'movie_age', 'studio_size', 'cast_size', 'director_count', 'writer_count', 'genre_count']\n",
        "pt = PowerTransformer(method='yeo-johnson')\n",
        "data[numerical_features] = data[numerical_features].fillna(0)\n",
        "data[numerical_features] = pt.fit_transform(data[numerical_features])\n",
        "\n",
        "# Prepare features\n",
        "target = data['audience_rating']\n",
        "non_text_features = data.drop(columns=['audience_rating', 'movie_title', 'movie_info', 'critics_consensus', 'genre', 'cast', 'directors', 'writers', 'studio_name'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qTYdJ4nTwJmL"
      },
      "outputs": [],
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(non_text_features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Process text features\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', str(text))\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "text_data = data[['movie_title', 'movie_info', 'critics_consensus', 'genre', 'cast']]\n",
        "text_data = text_data.fillna('')\n",
        "combined_texts = [\n",
        "    f\"TITLE: {preprocess_text(row['movie_title'])} INFO: {preprocess_text(row['movie_info'])} CONSENSUS: {preprocess_text(row['critics_consensus'])} GENRE: {preprocess_text(row['genre'])} CAST: {preprocess_text(row['cast'])}\"\n",
        "    for _, row in text_data.iterrows()\n",
        "]\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "text_features = tfidf.fit_transform(combined_texts)\n",
        "\n",
        "X_train_text, X_test_text, _, _ = train_test_split(text_features, target, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXhcRegdwi73",
        "outputId": "8c9a774f-3206-4209-92d7-50ada38010cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - loss: 32.0502 - mae: 4.3457 - val_loss: 13.4272 - val_mae: 3.0056\n",
            "Epoch 2/20\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - loss: 13.2201 - mae: 2.9635 - val_loss: 13.7270 - val_mae: 3.0548\n",
            "Epoch 3/20\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - loss: 11.1761 - mae: 2.7187 - val_loss: 14.2664 - val_mae: 3.1110\n",
            "Epoch 4/20\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - loss: 7.6559 - mae: 2.2093 - val_loss: 14.4469 - val_mae: 3.0502\n",
            "Epoch 5/20\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - loss: 5.1384 - mae: 1.8069 - val_loss: 13.8931 - val_mae: 3.0140\n",
            "Epoch 6/20\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - loss: 4.0142 - mae: 1.5860 - val_loss: 16.3611 - val_mae: 3.3313\n",
            "Epoch 7/20\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - loss: 3.6708 - mae: 1.5219 - val_loss: 14.5792 - val_mae: 3.1217\n",
            "Epoch 8/20\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - loss: 3.0191 - mae: 1.3610 - val_loss: 14.2142 - val_mae: 3.0716\n",
            "Epoch 9/20\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - loss: 2.8006 - mae: 1.3149 - val_loss: 13.7188 - val_mae: 3.0009\n",
            "Epoch 10/20\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 27ms/step - loss: 2.6665 - mae: 1.2786 - val_loss: 14.2477 - val_mae: 3.1031\n",
            "Epoch 11/20\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 28ms/step - loss: 2.3902 - mae: 1.2178 - val_loss: 14.4393 - val_mae: 3.1221\n",
            "Epoch 12/20\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - loss: 2.3666 - mae: 1.2105 - val_loss: 14.1416 - val_mae: 3.0756\n",
            "Epoch 13/20\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - loss: 2.1426 - mae: 1.1481 - val_loss: 13.5466 - val_mae: 3.0004\n",
            "Epoch 14/20\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - loss: 2.0043 - mae: 1.1237 - val_loss: 13.2317 - val_mae: 2.9515\n",
            "Epoch 15/20\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - loss: 2.0385 - mae: 1.1207 - val_loss: 14.1990 - val_mae: 3.0966\n",
            "Epoch 16/20\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 30ms/step - loss: 1.8358 - mae: 1.0705 - val_loss: 13.7643 - val_mae: 3.0607\n",
            "Epoch 17/20\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 29ms/step - loss: 1.7183 - mae: 1.0306 - val_loss: 14.6981 - val_mae: 3.1720\n",
            "Epoch 18/20\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - loss: 1.6774 - mae: 1.0381 - val_loss: 13.8464 - val_mae: 3.0451\n",
            "Epoch 19/20\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - loss: 1.5612 - mae: 0.9873 - val_loss: 13.8393 - val_mae: 3.0685\n",
            "Epoch 20/20\n",
            "\u001b[1m333/333\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 31ms/step - loss: 1.4198 - mae: 0.9381 - val_loss: 15.0567 - val_mae: 3.2163\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d8ea8b67750>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train models\n",
        "rf = RandomForestRegressor(n_estimators=200, max_depth=20, random_state=42)\n",
        "xgb = XGBRegressor(random_state=42)\n",
        "cat = CatBoostRegressor(random_state=42, verbose=0)\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "xgb.fit(X_train, y_train)\n",
        "cat.fit(X_train, y_train)\n",
        "\n",
        "# Train neural network for text features\n",
        "text_model = Sequential([\n",
        "    Dense(1024, activation='relu', input_shape=(1000,)),\n",
        "    Dropout(0.4),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "text_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "text_model.fit(X_train_text.toarray(), y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBPNYmz6fA4u",
        "outputId": "4d00d77d-6e1e-4f17-b808-c839ea64de84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "Evaluation Results:\n",
            "Mean Squared Error (MSE): 1.4927054643630981\n",
            "Mean Absolute Error (MAE): 0.9017853140830994\n",
            "R^2 Score: 0.9098700284957886\n",
            "Root Mean Squared Error (RMSE): 1.2217632603590183\n"
          ]
        }
      ],
      "source": [
        "# Generate predictions\n",
        "text_predictions = text_model.predict(X_test_text.toarray()).squeeze()\n",
        "non_text_predictions = [rf.predict(X_test), xgb.predict(X_test), cat.predict(X_test)]\n",
        "\n",
        "# Ensemble stacking\n",
        "meta_features = np.vstack([text_predictions, *non_text_predictions]).T\n",
        "meta_model = XGBRegressor(random_state=42, n_estimators=100)\n",
        "meta_model.fit(meta_features, y_test)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred = meta_model.predict(meta_features)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"R^2 Score: {r2}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEB8nQbB1UGy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
